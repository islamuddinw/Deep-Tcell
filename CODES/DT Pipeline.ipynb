{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a51d429-8031-4fce-9755-65200c5da686",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Decision Tree (DT) Pipeline for Tumor T-Cell Antigen Classification â€” Hardened\n",
    "-----------------------------------------------------------------------------\n",
    "- Loads CSV; expects a binary label column (default: 'label').\n",
    "- Robust handling for string labels (explicit --label_map or 2-class auto-map).\n",
    "- Uses ONLY numeric features (drops non-numeric with a warning).\n",
    "- 80/20 stratified train/holdout split.\n",
    "- Stratified K-Fold CV (auto-reduces folds if the minority class is tiny).\n",
    "- **No scaling** (trees are scale-invariant).\n",
    "- Optional SMOTE on training folds only (and final train) with adaptive k_neighbors.\n",
    "- DecisionTreeClassifier with predict_proba for ROC/AUC.\n",
    "- Metrics: ACC, SN, SP, AUC, MCC.\n",
    "- Saves: cv_fold_metrics.csv, cv_summary.csv, holdout_metrics.json,\n",
    "         confusion_matrix.png, roc_curve.png, holdout_predictions.csv,\n",
    "         and model (joblib) with feature names & metadata.\n",
    "\n",
    "Run examples:\n",
    "  python dt_pipeline.py --csv_path data.csv --label_col label --out_dir ./dt_outputs\n",
    "  python dt_pipeline.py --csv_path data.csv --label_map '{\"neg\":0,\"pos\":1}' \\\n",
    "      --class_weight balanced --max_depth 8 --min_samples_leaf 5 --ccp_alpha 0.0\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import warnings\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    roc_auc_score,\n",
    "    matthews_corrcoef,\n",
    "    confusion_matrix,\n",
    "    roc_curve,\n",
    ")\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from joblib import dump\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ===============================\n",
    "# Helpers\n",
    "# ===============================\n",
    "\n",
    "def ensure_out_dir(path: str):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "\n",
    "def to_bool(x: str) -> bool:\n",
    "    return str(x).lower() in {\"1\", \"true\", \"yes\", \"y\", \"t\"}\n",
    "\n",
    "\n",
    "def safe_confusion_matrix(y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    if cm.shape != (2, 2):\n",
    "        fixed = np.zeros((2, 2), dtype=int)\n",
    "        fixed[: cm.shape[0], : cm.shape[1]] = cm\n",
    "        cm = fixed\n",
    "    return cm\n",
    "\n",
    "\n",
    "def compute_metrics(y_true: np.ndarray, y_prob: np.ndarray) -> Dict[str, float]:\n",
    "    y_pred = (y_prob >= 0.5).astype(int)\n",
    "    cm = safe_confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    acc = (tp + tn) / max(tp + tn + fp + fn, 1)\n",
    "    sn = tp / (tp + fn + 1e-9)\n",
    "    sp = tn / (tn + fp + 1e-9)\n",
    "    try:\n",
    "        auc = roc_auc_score(y_true, y_prob)\n",
    "    except ValueError:\n",
    "        auc = float(\"nan\")\n",
    "    mcc = matthews_corrcoef(y_true, y_pred) if (tp + tn + fp + fn) > 0 else 0.0\n",
    "    return {\"ACC\": acc, \"SN\": sn, \"SP\": sp, \"AUC\": auc, \"MCC\": mcc}\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm: np.ndarray, out_path: str, class_names: List[str]):\n",
    "    fig, ax = plt.subplots(figsize=(6, 5))\n",
    "    _ = ax.imshow(cm, interpolation=\"nearest\", cmap=\"Blues\")\n",
    "    ax.set_title(\"Confusion Matrix (Decision Tree)\")\n",
    "    ax.set_xticks([0, 1]); ax.set_yticks([0, 1])\n",
    "    ax.set_xticklabels([f\"Predicted {c}\" for c in class_names], rotation=15, ha=\"right\")\n",
    "    ax.set_yticklabels([f\"Actual {c}\" for c in class_names])\n",
    "    for (i, j), val in np.ndenumerate(cm):\n",
    "        ax.text(j, i, f\"{int(val)}\", ha=\"center\", va=\"center\", fontsize=12)\n",
    "    fig.tight_layout(); fig.savefig(out_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def plot_roc(y_true: np.ndarray, y_score: np.ndarray, out_path: str, label=\"Decision Tree\"):\n",
    "    try:\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "        auc = roc_auc_score(y_true, y_score)\n",
    "    except ValueError:\n",
    "        fpr, tpr, auc = [0, 1], [0, 1], float(\"nan\")\n",
    "    fig, ax = plt.subplots(figsize=(6, 5))\n",
    "    ax.plot(fpr, tpr, linewidth=2, label=f\"{label} (AUC={auc:.2f})\")\n",
    "    ax.plot([0, 1], [0, 1], \"--\", linewidth=1)\n",
    "    ax.set_xlabel(\"False Positive Rate\"); ax.set_ylabel(\"True Positive Rate\")\n",
    "    ax.set_title(\"ROC Curve (Holdout)\"); ax.legend(loc=\"lower right\")\n",
    "    fig.tight_layout(); fig.savefig(out_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def pick_numeric_features(df: pd.DataFrame, label_col: str) -> Tuple[pd.DataFrame, List[str]]:\n",
    "    feature_df = df.drop(columns=[label_col])\n",
    "    numeric_df = feature_df.select_dtypes(include=[np.number])\n",
    "    dropped = sorted(set(feature_df.columns) - set(numeric_df.columns))\n",
    "    if dropped:\n",
    "        print(f\"[Warn] Dropping non-numeric columns (not used as features): {dropped}\")\n",
    "    return numeric_df, list(numeric_df.columns)\n",
    "\n",
    "\n",
    "def parse_class_names(arg: Optional[str]) -> List[str]:\n",
    "    default = [\"Non-Tumor\", \"Tumor\"]\n",
    "    if not arg:\n",
    "        return default\n",
    "    parts = [p.strip() for p in arg.split(\",\")]\n",
    "    if len(parts) != 2:\n",
    "        print(f\"[Warn] --class_names expects exactly two names; using default {default}.\")\n",
    "        return default\n",
    "    return parts\n",
    "\n",
    "\n",
    "def minority_count(y: np.ndarray) -> int:\n",
    "    uniq, counts = np.unique(y, return_counts=True)\n",
    "    return int(counts.min()) if len(counts) else 0\n",
    "\n",
    "\n",
    "def best_smote(y: np.ndarray, seed: int) -> SMOTE:\n",
    "    m = minority_count(y)\n",
    "    k = max(1, min(5, m - 1))\n",
    "    return SMOTE(random_state=seed, k_neighbors=k)\n",
    "\n",
    "\n",
    "def auto_folds(y_tr: np.ndarray, desired: int = 5) -> int:\n",
    "    m = minority_count(y_tr)\n",
    "    return max(2, min(desired, m))\n",
    "\n",
    "# ===============================\n",
    "# Core Pipeline\n",
    "# ===============================\n",
    "\n",
    "def run(\n",
    "    csv_path: str,\n",
    "    label_col: str = \"label\",\n",
    "    label_map: str = None,\n",
    "    out_dir: str = \"./dt_outputs\",\n",
    "    use_smote: bool = True,\n",
    "    class_weight: str = \"none\",  # 'balanced' or 'none'\n",
    "    criterion: str = \"gini\",      # 'gini' or 'entropy' or 'log_loss'\n",
    "    max_depth: Optional[int] = None,\n",
    "    min_samples_split: int = 2,\n",
    "    min_samples_leaf: int = 1,\n",
    "    max_features: Optional[str] = None,  # 'sqrt', 'log2', None, or int/float\n",
    "    ccp_alpha: float = 0.0,              # cost-complexity pruning\n",
    "    class_names_arg: Optional[str] = None,\n",
    "    seed: int = 42,\n",
    "):\n",
    "    np.random.seed(seed)\n",
    "    ensure_out_dir(out_dir)\n",
    "\n",
    "    # Load data\n",
    "    df = pd.read_csv(csv_path)\n",
    "    if label_col not in df.columns:\n",
    "        raise ValueError(f\"Label column '{label_col}' not found in CSV.\")\n",
    "\n",
    "    # Labels\n",
    "    y_raw = df[label_col]\n",
    "    if label_map:\n",
    "        mapping = json.loads(label_map)\n",
    "        y = y_raw.map(mapping)\n",
    "    else:\n",
    "        if pd.api.types.is_numeric_dtype(y_raw):\n",
    "            y = y_raw\n",
    "        else:\n",
    "            uniq = y_raw.dropna().unique()\n",
    "            if len(uniq) == 2:\n",
    "                keys = sorted(list(uniq), key=lambda v: str(v))\n",
    "                mapping = {keys[0]: 0, keys[1]: 1}\n",
    "                print(f\"[Info] Auto label_map inferred: {mapping}\")\n",
    "                y = y_raw.map(mapping)\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"Label column is non-numeric and has !=2 unique values. \"\n",
    "                    \"Provide --label_map, e.g. '{\\\"neg\\\":0,\\\"pos\\\":1}'.\"\n",
    "                )\n",
    "    if y.isna().any():\n",
    "        raise ValueError(\"Label mapping produced NaNs. Check --label_map and label values.\")\n",
    "    y = y.astype(int).values\n",
    "\n",
    "    # Features (numeric only)\n",
    "    X_df, feature_names = pick_numeric_features(df, label_col)\n",
    "    X = X_df.values.astype(np.float32)\n",
    "\n",
    "    print(f\"[Info] Data: X={X.shape}, positives={int(y.sum())}, negatives={int((y==0).sum())}\")\n",
    "\n",
    "    # Holdout split\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "        X, y, test_size=0.2, stratify=y, random_state=seed\n",
    "    )\n",
    "    print(f\"[Info] Holdout split => train={X_tr.shape[0]}, test={X_te.shape[0]}\")\n",
    "\n",
    "    # CV\n",
    "    n_splits = auto_folds(y_tr, desired=5)\n",
    "    if n_splits < 5:\n",
    "        print(f\"[Warn] Reduced CV folds to {n_splits} due to limited minority samples.\")\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "\n",
    "    cv_metrics = []\n",
    "\n",
    "    cw = None if str(class_weight).lower() == \"none\" else \"balanced\"\n",
    "\n",
    "    for fold, (idx_tr, idx_va) in enumerate(skf.split(X_tr, y_tr), start=1):\n",
    "        X_tr_fold, X_va_fold = X_tr[idx_tr], X_tr[idx_va]\n",
    "        y_tr_fold, y_va_fold = y_tr[idx_tr], y_tr[idx_va]\n",
    "\n",
    "        # SMOTE on training fold only (optional)\n",
    "        if use_smote:\n",
    "            sm = best_smote(y_tr_fold, seed)\n",
    "            X_tr_fold, y_tr_fold = sm.fit_resample(X_tr_fold, y_tr_fold)\n",
    "            print(f\"[Fold {fold}] After SMOTE: X={X_tr_fold.shape}, pos={int(y_tr_fold.sum())}\")\n",
    "\n",
    "        clf = DecisionTreeClassifier(\n",
    "            criterion=criterion,\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=min_samples_split,\n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "            max_features=max_features,\n",
    "            class_weight=cw,\n",
    "            ccp_alpha=ccp_alpha,\n",
    "            random_state=seed,\n",
    "        )\n",
    "        clf.fit(X_tr_fold, y_tr_fold)\n",
    "\n",
    "        y_va_prob = clf.predict_proba(X_va_fold)[:, 1]\n",
    "        fold_m = compute_metrics(y_va_fold, y_va_prob)\n",
    "        cv_metrics.append(fold_m)\n",
    "        print(f\"[Fold {fold}] {fold_m}\")\n",
    "\n",
    "    # CV summary\n",
    "    cv_df = pd.DataFrame(cv_metrics)\n",
    "    cv_df.to_csv(os.path.join(out_dir, \"data.csv\"), index=False)\n",
    "    cv_summary = cv_df.agg([\"mean\", \"std\"]).T\n",
    "    cv_summary.to_csv(os.path.join(out_dir, \"cv_summary.csv\"))\n",
    "    print(\"\\n[CV Summary]\\n\", cv_summary)\n",
    "\n",
    "    # Final train & holdout eval\n",
    "    if use_smote:\n",
    "        sm = best_smote(y_tr, seed)\n",
    "        X_tr_bal, y_tr_bal = sm.fit_resample(X_tr, y_tr)\n",
    "    else:\n",
    "        X_tr_bal, y_tr_bal = X_tr, y_tr\n",
    "\n",
    "    clf_f = DecisionTreeClassifier(\n",
    "        criterion=criterion,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        max_features=max_features,\n",
    "        class_weight=cw,\n",
    "        ccp_alpha=ccp_alpha,\n",
    "        random_state=seed,\n",
    "    )\n",
    "    clf_f.fit(X_tr_bal, y_tr_bal)\n",
    "\n",
    "    y_te_prob = clf_f.predict_proba(X_te)[:, 1]\n",
    "    holdout = compute_metrics(y_te, y_te_prob)\n",
    "    with open(os.path.join(out_dir, \"holdout_metrics.json\"), \"w\") as f:\n",
    "        json.dump(holdout, f, indent=2)\n",
    "    print(\"\\n[Holdout Metrics]\\n\", holdout)\n",
    "\n",
    "    # Plots\n",
    "    y_te_pred = (y_te_prob >= 0.5).astype(int)\n",
    "    cm = safe_confusion_matrix(y_te, y_te_pred)\n",
    "    class_names = parse_class_names(class_names_arg)\n",
    "\n",
    "    plot_confusion_matrix(cm, os.path.join(out_dir, \"confusion_matrix.png\"), class_names)\n",
    "    plot_roc(y_te, y_te_prob, os.path.join(out_dir, \"roc_curve.png\"), label=\"Decision Tree\")\n",
    "\n",
    "    # Save artifacts\n",
    "    artifact = {\n",
    "        \"dt\": clf_f,\n",
    "        \"feature_names\": feature_names,\n",
    "        \"label_col\": label_col,\n",
    "        \"class_names\": class_names,\n",
    "        \"params\": {\n",
    "            \"criterion\": criterion,\n",
    "            \"max_depth\": max_depth,\n",
    "            \"min_samples_split\": min_samples_split,\n",
    "            \"min_samples_leaf\": min_samples_leaf,\n",
    "            \"max_features\": max_features,\n",
    "            \"class_weight\": cw,\n",
    "            \"ccp_alpha\": ccp_alpha,\n",
    "            \"use_smote\": use_smote,\n",
    "        },\n",
    "    }\n",
    "    dump(artifact, os.path.join(out_dir, \"dt_model.joblib\"))\n",
    "\n",
    "    # Save holdout predictions\n",
    "    pd.DataFrame({\n",
    "        \"y_true\": y_te,\n",
    "        \"y_prob\": y_te_prob,\n",
    "        \"y_pred\": y_te_pred,\n",
    "    }).to_csv(os.path.join(out_dir, \"holdout_predictions.csv\"), index=False)\n",
    "\n",
    "    print(f\"[Done] Outputs saved to: {out_dir}\")\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# CLI\n",
    "# ===============================\n",
    "\n",
    "def build_argparser():\n",
    "    ap = argparse.ArgumentParser(description=\"Decision Tree pipeline with optional SMOTE and ROC plotting (hardened).\")\n",
    "    ap.add_argument(\"--csv_path\", type=str, required=True, help=\"Path to local CSV file.\")\n",
    "    ap.add_argument(\"--label_col\", type=str, default=\"label\", help=\"Name of label column (0/1).\")\n",
    "    ap.add_argument(\"--label_map\", type=str, default=None, help='Optional JSON mapping, e.g. {\"neg\":0,\"pos\":1}')\n",
    "    ap.add_argument(\"--out_dir\", type=str, default=\"./dt_outputs\", help=\"Output directory.\")\n",
    "    ap.add_argument(\"--smote\", type=str, default=\"true\", help=\"Apply SMOTE on train folds (true/false).\")\n",
    "    ap.add_argument(\"--class_weight\", type=str, default=\"none\", choices=[\"none\", \"balanced\"], help='Class weighting for imbalance.')\n",
    "    ap.add_argument(\"--criterion\", type=str, default=\"gini\", choices=[\"gini\", \"entropy\", \"log_loss\"], help=\"Split criterion.\")\n",
    "    ap.add_argument(\"--max_depth\", type=int, default=None, help=\"Max depth of the tree (None for full growth).\")\n",
    "    ap.add_argument(\"--min_samples_split\", type=int, default=2, help=\"Min samples to split an internal node.\")\n",
    "    ap.add_argument(\"--min_samples_leaf\", type=int, default=1, help=\"Min samples at a leaf node.\")\n",
    "    ap.add_argument(\"--max_features\", type=str, default=None, help=\"Number of features to consider when looking for the best split (e.g., 'sqrt', 'log2', None).\")\n",
    "    ap.add_argument(\"--ccp_alpha\", type=float, default=0.0, help=\"Complexity parameter used for Minimal Cost-Complexity Pruning.\")\n",
    "    ap.add_argument(\"--class_names\", type=str, default=None, help='Comma-separated names for classes in plots, e.g. \"Non-Tumor,Tumor\"')\n",
    "    ap.add_argument(\"--seed\", type=int, default=42, help=\"Random seed.\")\n",
    "    return ap\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = build_argparser().parse_args()\n",
    "    run(\n",
    "        csv_path=args.csv_path,\n",
    "        label_col=args.label_col,\n",
    "        label_map=args.label_map,\n",
    "        out_dir=args.out_dir,\n",
    "        use_smote=to_bool(args.smote),\n",
    "        class_weight=args.class_weight,\n",
    "        criterion=args.criterion,\n",
    "        max_depth=args.max_depth,\n",
    "        min_samples_split=args.min_samples_split,\n",
    "        min_samples_leaf=args.min_samples_leaf,\n",
    "        max_features=args.max_features,\n",
    "        ccp_alpha=args.ccp_alpha,\n",
    "        class_names_arg=args.class_names,\n",
    "        seed=args.seed,\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

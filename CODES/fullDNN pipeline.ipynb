{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d4323d-a345-40f2-9292-60746b70184a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Deep Neural Network (DNN) for Tumor T-Cell Antigen Classification\n",
    "---------------------------------------------------------------\n",
    "This script loads a CSV dataset, preprocesses it, applies SMOTE,\n",
    "performs feature extraction, trains a DNN, evaluates with CV, and\n",
    "reports performance metrics with plots.\n",
    "\"\"\"\n",
    "\n",
    "# ===============================\n",
    "# Imports\n",
    "# ===============================\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import warnings\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (accuracy_score, roc_auc_score, matthews_corrcoef,\n",
    "                             confusion_matrix, roc_curve)\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers, callbacks\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# Utility functions\n",
    "# ===============================\n",
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "\n",
    "def ensure_out_dir(out_dir: str):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "def compute_metrics(y_true: np.ndarray, y_prob: np.ndarray) -> Dict[str, float]:\n",
    "    \"\"\"Compute performance metrics.\"\"\"\n",
    "    y_pred = (y_prob >= 0.5).astype(int)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    sn = tp / (tp + fn + 1e-9)\n",
    "    sp = tn / (tn + fp + 1e-9)\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "    mcc = matthews_corrcoef(y_true, y_pred)\n",
    "    return {\"ACC\": acc, \"SN\": sn, \"SP\": sp, \"AUC\": auc, \"MCC\": mcc}\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm: np.ndarray, out_path: str, class_names=[\"Non-Tumor\", \"Tumor\"]):\n",
    "    fig, ax = plt.subplots(figsize=(6, 5))\n",
    "    im = ax.imshow(cm, interpolation=\"nearest\", cmap=\"Blues\")\n",
    "    ax.set_title(\"Confusion Matrix\")\n",
    "    ax.set_xticks([0, 1])\n",
    "    ax.set_yticks([0, 1])\n",
    "    ax.set_xticklabels([f\"Predicted {c}\" for c in class_names], rotation=15, ha=\"right\")\n",
    "    ax.set_yticklabels([f\"Actual {c}\" for c in class_names])\n",
    "    for (i, j), val in np.ndenumerate(cm):\n",
    "        ax.text(j, i, f\"{int(val)}\", ha=\"center\", va=\"center\", fontsize=12)\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(out_path, dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def plot_roc_curve(y_true: np.ndarray, y_score: np.ndarray, out_path: str, label: str = \"DNN\"):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "    auc = roc_auc_score(y_true, y_score)\n",
    "    fig, ax = plt.subplots(figsize=(6, 5))\n",
    "    ax.plot(fpr, tpr, linewidth=2, label=f\"{label} (AUC={auc:.2f})\")\n",
    "    ax.plot([0, 1], [0, 1], \"--\", linewidth=1)\n",
    "    ax.set_xlabel(\"False Positive Rate\")\n",
    "    ax.set_ylabel(\"True Positive Rate\")\n",
    "    ax.set_title(\"ROC Curve\")\n",
    "    ax.legend(loc=\"lower right\")\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(out_path, dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# Model definition\n",
    "# ===============================\n",
    "def build_dnn(input_dim: int, l2_reg: float = 0.01, dropout: float = 0.5) -> keras.Model:\n",
    "    \"\"\"Builds a simple DNN classifier.\"\"\"\n",
    "    inp = layers.Input(shape=(input_dim,), name=\"input\")\n",
    "    x = layers.Dense(256, activation=\"relu\",\n",
    "                     kernel_regularizer=regularizers.l2(l2_reg))(inp)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Dense(128, activation=\"relu\",\n",
    "                     kernel_regularizer=regularizers.l2(l2_reg))(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.Dense(64, activation=\"relu\",\n",
    "                     kernel_regularizer=regularizers.l2(l2_reg))(x)\n",
    "    out = layers.Dense(1, activation=\"sigmoid\", name=\"output\")(x)\n",
    "\n",
    "    model = keras.Model(inputs=inp, outputs=out, name=\"DNN\")\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "                  loss=\"binary_crossentropy\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# Main Training Pipeline\n",
    "# ===============================\n",
    "def run_pipeline(csv_path: str,\n",
    "                 label_col: str = \"label\",\n",
    "                 seq_col: str = None,\n",
    "                 out_dir: str = \"./outputs\",\n",
    "                 epochs: int = 50,\n",
    "                 batch_size: int = 32,\n",
    "                 use_smote: bool = True,\n",
    "                 seed: int = 42):\n",
    "\n",
    "    seed_everything(seed)\n",
    "    ensure_out_dir(out_dir)\n",
    "\n",
    "    # 1. Load CSV\n",
    "    df = pd.read_csv(\"Dataset\")\n",
    "    if label_col not in df.columns:\n",
    "        raise ValueError(f\"Label column '{label_col}' not found in CSV.\")\n",
    "\n",
    "    y = df[label_col].astype(int).values\n",
    "    X = df.drop(columns=[label_col]).values.astype(np.float32)\n",
    "\n",
    "    print(f\"[Info] Dataset: X={X.shape}, y={y.shape}, positive={y.sum()}, negative={len(y)-y.sum()}\")\n",
    "\n",
    "    # 2. Train/test split\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "        X, y, test_size=0.2, stratify=y, random_state=seed\n",
    "    )\n",
    "\n",
    "    # 3. Cross-validation on training set\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    cv_metrics = []\n",
    "\n",
    "    for fold, (idx_tr, idx_va) in enumerate(skf.split(X_tr, y_tr), start=1):\n",
    "        X_tr_fold, X_va_fold = X_tr[idx_tr], X_tr[idx_va]\n",
    "        y_tr_fold, y_va_fold = y_tr[idx_tr], y_tr[idx_va]\n",
    "\n",
    "        # Scaling\n",
    "        scaler = StandardScaler()\n",
    "        X_tr_fold = scaler.fit_transform(X_tr_fold)\n",
    "        X_va_fold = scaler.transform(X_va_fold)\n",
    "\n",
    "        # SMOTE\n",
    "        if use_smote:\n",
    "            sm = SMOTE(random_state=seed)\n",
    "            X_tr_fold, y_tr_fold = sm.fit_resample(X_tr_fold, y_tr_fold)\n",
    "\n",
    "        # Model\n",
    "        model = build_dnn(input_dim=X_tr_fold.shape[1])\n",
    "\n",
    "        cb = [\n",
    "            callbacks.EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True),\n",
    "            callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=5)\n",
    "        ]\n",
    "\n",
    "        model.fit(X_tr_fold, y_tr_fold,\n",
    "                  validation_data=(X_va_fold, y_va_fold),\n",
    "                  epochs=epochs, batch_size=batch_size,\n",
    "                  verbose=0, callbacks=cb)\n",
    "\n",
    "        y_va_prob = model.predict(X_va_fold, verbose=0).ravel()\n",
    "        fold_metrics = compute_metrics(y_va_fold, y_va_prob)\n",
    "        cv_metrics.append(fold_metrics)\n",
    "        print(f\"[Fold {fold}] {fold_metrics}\")\n",
    "\n",
    "    # CV Summary\n",
    "    cv_df = pd.DataFrame(cv_metrics)\n",
    "    cv_summary = cv_df.agg([\"mean\", \"std\"]).T\n",
    "    cv_summary.to_csv(os.path.join(out_dir, \"cv_summary.csv\"))\n",
    "    print(\"\\n[CV Summary]\")\n",
    "    print(cv_summary)\n",
    "\n",
    "    # 4. Train on full train set & evaluate on holdout\n",
    "    scaler = StandardScaler()\n",
    "    X_tr_scaled = scaler.fit_transform(X_tr)\n",
    "    X_te_scaled = scaler.transform(X_te)\n",
    "\n",
    "    if use_smote:\n",
    "        sm = SMOTE(random_state=seed)\n",
    "        X_tr_scaled, y_tr = sm.fit_resample(X_tr_scaled, y_tr)\n",
    "\n",
    "    model_final = build_dnn(input_dim=X_tr_scaled.shape[1])\n",
    "\n",
    "    cb_final = [\n",
    "        callbacks.EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True),\n",
    "        callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=5)\n",
    "    ]\n",
    "\n",
    "    model_final.fit(X_tr_scaled, y_tr,\n",
    "                    validation_split=0.1,\n",
    "                    epochs=epochs, batch_size=batch_size,\n",
    "                    verbose=0, callbacks=cb_final)\n",
    "\n",
    "    y_te_prob = model_final.predict(X_te_scaled, verbose=0).ravel()\n",
    "    holdout_metrics = compute_metrics(y_te, y_te_prob)\n",
    "    print(\"\\n[Holdout Metrics]\")\n",
    "    print(holdout_metrics)\n",
    "\n",
    "    # Save holdout metrics\n",
    "    with open(os.path.join(out_dir, \"holdout_metrics.json\"), \"w\") as f:\n",
    "        json.dump(holdout_metrics, f, indent=2)\n",
    "\n",
    "    # Plots\n",
    "    y_te_pred = (y_te_prob >= 0.5).astype(int)\n",
    "    cm = confusion_matrix(y_te, y_te_pred, labels=[0, 1])\n",
    "    plot_confusion_matrix(cm, os.path.join(out_dir, \"confusion_matrix.png\"))\n",
    "    plot_roc_curve(y_te, y_te_prob, os.path.join(out_dir, \"roc_curve.png\"))\n",
    "\n",
    "    # Save model\n",
    "    model_final.save(os.path.join(out_dir, \"dnn_model.h5\"))\n",
    "    print(f\"\\n[Done] Outputs saved in: {out_dir}\")\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# Run\n",
    "# ===============================\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    run_pipeline(\n",
    "        csv_path=\"your_dataset.csv\",  # <-- Replace with your CSV path\n",
    "        label_col=\"label\",\n",
    "        out_dir=\"./outputs\",\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        use_smote=True\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
